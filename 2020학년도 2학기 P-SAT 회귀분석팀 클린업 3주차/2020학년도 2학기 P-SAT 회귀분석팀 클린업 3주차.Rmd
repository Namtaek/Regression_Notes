---
title: "Clean-Up Week 3"
output: html_document
---

# **회귀분석팀 클린업 3주차 R 예제**

2주차에서 썻던 데이터 가져와서 하자!

```{r}
library(data.table)
library(tidyverse)

data = fread('2017-18_NBA_salary.csv')
```

데이터 확인

```{r}
data %>% dim
data %>% glimpse
```



```{r}
data <- data[complete.cases(data), ] # 결측치 제거
data <- data[-c(223:224), ] #겹치는 선수 삭제
data$Player <- NULL  
data$Tm <- NULL
data$NBA_Country <- NULL

data = data %>% select('Salary', 'NBA_DraftNumber', 'Age', 'MP', 'PER', 
                       `TS%`, `DRB%`, VORP, G, OBPM, DBPM, BPM, OWS, DWS, WS, `AST%`, `STL%`)
```

원래 중요하다고 나온 변수들에 일종의 노이즈를 좀 주었습니다! 안중요하다고 모델이 말한 변수들도 넣어서 만듬! 

또한 PCR을 위해 범주형 변수 삭제함

```{r}
# train test split 진행
set.seed(1234)
train_idx <- caret::createDataPartition(data$Salary, p = .8, list = FALSE, times = 1)
train = data[train_idx, ]
test = data[-train_idx, ]

train$Salary = train$Salary^0.22
test$Salary = test$Salary^0.22
# 단위수정및 분포형태 고려

train_x = train %>% select(-Salary) %>% as.matrix()
train_y = train %>% select(Salary) %>% unlist %>% as.vector
test_x = test %>% select(-Salary) %>% as.matrix()
test_y = test %>% select(Salary) %>% unlist() %>% as.vector()

dim(train) # 387행 17열
dim(test) # 94행 17열
```

관측치에 비해 변수가 어느정도 있는 형태이다. 따라서 다중공선성이 의심된다.

```{r}
lm_fit = lm(Salary ~ ., data = train)
lm_fit %>% summary
```

회귀식에서 유의하지 않은 변수들이 많다. 다중공선성이 있는지를 확인해보자

```{r}
library(ggcorrplot)
corr <- cor(train) # 상관계수 행렬
p_mat <- cor_pmat(train) # 상관계수 p-value 행렬
ggcorrplot(corr, hc.order = T, type = "lower", 
           outline.color = "white", ggtheme = ggplot2::theme_gray(), 
           colors = c("#6D9EC1", "white", "#E46726"), lab = T, p.mat = p_mat,
           insig = "blank", lab_size = 2, tl.cex = 7)
```

상관계수가 높은 친구들이 많이 보인다.

```{r}
car::vif(lm_fit)
```

Vif 값이 상당히 높다. 단순히 무시할 수준이 아님을 짐작 가능하다!


### 1. 변수 선택법

변수선택법은 cv와 같은 방법을 사용하지 않고, 그냥 주어진 train을 한번에 활용해서 결론을 내는 방식을 주로 사용한다.

##### Best Subset Selection

```{r}
library(leaps)
best_subset = regsubsets(Salary ~ ., data = train, nvmax = 13)
bs = best_subset %>% summary

par(mfrow = c(2, 2))
plot(bs$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")
plot(bs$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq", type = "l")
l = which.max(bs$adjr2)
points(l, bs$adjr2[l], col = "red", cex = 2, pch = 20)
plot(bs$cp, xlab = "Number of Variables", ylab = "Cp", type = 'l')
l = which.min(bs$cp)
points(l, bs$cp[l], col = "red", cex = 2, pch = 20)
plot(bs$bic, xlab = "Number of Variables", ylab = "BIC", type = 'l')
l = which.min(bs$bic)
points(l, bs$bic[l], col = "red", cex = 2, pch = 20)
par(mfrow = c(1, 1))
```

변수가 이렇게 기준이 다르다. 그러면 그냥 변수 적은게 좋다고 나는 생각한다!

```{r}
coef(best_subset, 5)

best_lm_fit = lm(Salary ~ NBA_DraftNumber + Age + MP + VORP + G, data = train)
```

##### Forward Selection

```{r}
forward_selection = regsubsets(Salary ~ ., data = train, nvmax = 13, method = "forward")
summary(forward_selection)

fs = summary(forward_selection)
plot(fs$bic, xlab = "Number of Variables", ylab = "BIC", type = 'l')
l = which.min(fs$bic)
points(l, fs$bic[l], col = "red", cex = 2, pch = 20)

coef(forward_selection, 5)
```

##### Backward Elimination

```{r}
backward_selection = regsubsets(Salary ~ ., data = train, nvmax = 13, method = "forward")
summary(backward_selection)

coef(backward_selection, 5)
```

결과가 다르다. 방향이 다르니 이렇게 달라질 수 있는 것! 나중에 테스트 평가는 best subset selection으로 해보자



### Principal Component Regression

PCR에서는 주성분의 갯수를 튜닝해줘야 한다. 원래는 뒤의 릿지 라쏘처럼 CV하는게 좋지만....코드짜기 간단한 holdout으로 갈게요 ㅎㅎ

```{r}
set.seed(1234)
train_idx <- caret::createDataPartition(train$Salary, p = .8, list = FALSE, times = 1)

train_tune = data[train_idx, ]
val = data[-train_idx, ]
```

튜닝을 위한 셋과 validation set을 나눈다.

```{r}
library(pls)
library(MLmetrics)

pcr_fit = pcr(Salary ~ ., data = train_tune, scale = TRUE)
pcr_fit %>% summary

rmse_pcr_df = data.frame(rep(NA, 16), rep(NA, 16))
colnames(rmse_pcr_df) = c('PC', 'RMSE')
rmse_pcr_df %>% head

for (i in 1:dim(rmse_pcr_df)[1]) {
  pcr_fit = pcr(Salary ~ ., data = train_tune)
  pred_y = predict(pcr_fit, val, ncomp = i)
  rmse_pcr_df[i, 2] = RMSE(pred_y, val$Salary)
  rmse_pcr_df[i, 1] = paste('PC', i)
}

rmse_pcr_df
```

이렇게 돌렸습니다.

```{r}
plot(1:16, rmse_pcr_df$RMSE, type = 'b')
l = which.min(rmse_pcr_df$RMSE)
points(l, rmse_pcr_df$RMSE[l], col = "red", cex = 2, pch = 20)
```

10개가 최적의 주성분이라고 나온다. 너무 많은 감이 있다!

```{r}
pcr_fit = pcr(Salary ~ ., data = train, ncomp = 10)
pcr_fit %>% summary
```

pcr_fit을 좀있다가 비교해보자!


### ridge regression

```{r}
library(glmnet)
# if alpha=0 -> ridge; if alpha=1 -> lasso.
# Default: standardize = TRUE
```

ridge 가볼까?

```{r}
grid_lambda = seq(from = 0, to = 10, length.out = 200)
grid_lambda

cv_ridge_fit = cv.glmnet(x = train_x, y = train_y, alpha = 0, lambda = grid_lambda, nfolds = 5)
opt_ridge_lambda = cv_ridge_fit$lambda.min ; opt_ridge_lambda
cv_ridge_fit %>% plot
cv_ridge_fit %>% coef
```

0인 계수가 없다!!!

```{r}
ridge_fit = glmnet(x = train_x, y = train_y, alpha = 0, lambda = opt_ridge_lambda)
ridge_fit %>% coef
```


### Lasso regression

```{r}
grid_lambda = seq(from = 0, to = 10, length.out = 200)
grid_lambda

cv_lasso_fit = cv.glmnet(x = train_x, y = train_y, alpha = 1, lambda = grid_lambda, nfolds = 5)
opt_lasso_lambda = cv_lasso_fit$lambda.min ; opt_lasso_lambda 
cv_lasso_fit %>% plot
cv_lasso_fit %>% coef
```

0인거 보이지?

```{r}
lasso_fit = glmnet(x = train_x, y = train_y, alpha = 1, lambda = opt_ridge_lambda)
lasso_fit %>% coef
```


### **결과 비교!!!!**

```{r}
pred_bs_y = predict(best_lm_fit, test)
pred_pcr_y = predict(pcr_fit, test)
pred_ridge_y = predict(ridge_fit, s = opt_ridge_lambda, newx = test_x)
pred_lasso_y = predict(lasso_fit, s = opt_lasso_lambda, newx = test_x)
```

각각 예측값 저장하고,

```{r}
RMSE(pred_bs_y, test_y)
RMSE(pred_pcr_y, test_y)
RMSE(pred_ridge_y, test_y)
RMSE(pred_lasso_y, test_y)
```

결과가 이렇다!